---
layout: post
title: EIA data and R
author: Petr Baca
published: true
status: publish
draft: false
tags: R
---
##### EIA data and R
 
THe amount of data available via web services has been soaring in the recent years. One of the organisations that decided to make its data publicly accessible via web is the [Energy Information Administration](http://www.eia.gov) (EIA). The EIA is in fact something like a statistical arm of the US Department of Energy and its task is, among other things, to create closely followed analyses such as the Short Term Energy Outlook ([STEO](http://www.eia.gov/forecasts/steo/report/)). For this reason, the EIA also collects a huge amount of energy-related data. And this data is now available via the API.
 
##### EIA API? Just great!
 
[API](http://en.wikipedia.org/wiki/Application_programming_interface), or Application Programming Interface, maintained by the EIA is simply great! Despite being "just" a beta-version and as such suffering from some childhood diseases, it is already a very useful source of information. After a (free) registration, any user can obtain a unique API key and is then able to download [XML](https://en.wikipedia.org/wiki/XML) data files directly from the web via [REST](http://en.wikipedia.org/wiki/Representational_state_transfer)ful services.
 
What does it mean and how does it work? From the user's point of view, RESTful services are in fact quite simple. All that needs to be done is to send a 'database query' in a form of a particular 'web address'. That is, it reminds simple writing 'www.google.com' into a web browser's window. As a response, the server returns an XML file that contains the data of interest.
 
After getting the data, the user has to extract the information he or she is looking for. To get this job done, we will use [R](http://www.r-project.org/). The reason is simple as the user usually wants to perform some sort of analysis with the data and R is a great tool for such a goal.
 
##### EIA & R = <3
 
R undoubtedly is one of the best tools for data analysis. In our case, one of the reasons to use R is that it supports [web technologies](http://cran.r-project.org/web/views/WebTechnologies.html). So, how should we proceed?
 
First, we will have to download some external libraries from the list above. Namely, we need __RCurl__ and __XML__ libraries. To be able to work with time series data more efficiently we will also get __xts__ library.
 
So, let's say we want to download monthly data on the US crude oil production. Anyone who has ever heard of "the shale revolution" would know that this kind of data has been attracting a lot of attention in the recent years.
 
After a while we can quite easily find [this](http://www.eia.gov/opendata/qb.cfm?category=296686&sdid=PET.MCRFPUS2.M) webpage. It contains all the information we need to know, in particular the code of the series (_PET.MCRFPUS2.M_). If we have the code, we can retrieve and process the data using the following script (warning: you have to use your own API key to be able to use the script).
 

 
First, we will activate the libraries.
 

{% highlight r %}
library("RCurl") # data retrieval
library("XML") # XML parsing
library("xts") # to be able to convert to a 'nice' time series object
{% endhighlight %}
 
The following code chunk shows how to actually get and process the data.
 

{% highlight r %}
# we will keep the URL (or URI) in variable URL
# don't forget to get your own API key
 
#URL <- "http://api.eia.gov/series/?api_key=YOUR_API_KEY_HERE&series_id=PET.MCRFPUS2.M&out=xml"
 
# data download and processing
XML <- getURL(URL, httpheader = list('User-Agent' = 'R-Agent'))
tree <- xmlTreeParse(XML, useInternalNodes=TRUE)
  
# get values of interest from the XML file
date <- xpathSApply(tree,"//series/row/data/row/date", xmlValue)
value <- xpathSApply(tree,"//series/row/data/row/value", function(x) as.numeric(xmlValue(x)))
 
# OK, now let's convert the data to time series object
# first, convert date so that R understands it really is date
d <- as.Date(paste(substr(date, 1, 4),
                   substr(date, 5, 6),
                   "01", sep = "-"))
 
# save the results to variable called 'prod'
prod <- xts(value, d); colnames(prod) <- "US_oil_prod"
 
# ...and show the data for the last 12 months (available)
tail(prod, 12)
{% endhighlight %}



{% highlight text %}
##            US_oil_prod
## 2015-02-01        9451
## 2015-03-01        9648
## 2015-04-01        9694
## 2015-05-01        9479
## 2015-06-01        9315
## 2015-07-01        9433
## 2015-08-01        9407
## 2015-09-01        9452
## 2015-10-01        9377
## 2015-11-01        9328
## 2015-12-01        9235
## 2016-01-01        9179
{% endhighlight %}
 
Now, we can easily plot the data we just downloaded...
 

{% highlight r %}
plot(prod, main = "Monthly US crude oil production", ylab = "thousand barrels per day")
{% endhighlight %}

![plot of chunk plot](/figures/plot-1.png)
 
You may be thinking. Why should I do it? Why should I learn how to write an R script when I could simply download the same data in an excel file or see the plot on EIA website?
 
Well, there are many reasons! The most important one probably is that the script above (or the report based on the script such as this blog post) is reproducible. That means you can run exactly the same script the next month, get a coffee (or rather take a look from the window for 5 seconds) and the new plot (or some more advanced analysis) is done!
 
Of course, you can do much more than just a simple plot in R. One obvious extension of the script above for example is to wrap its key parts into a function that can download the data for any time series. And as far as the comparison of analytical tools available in R and in Excel is concerned, needles to say that the former goes far beyond what you can do in an Excel spreadsheet...
 
